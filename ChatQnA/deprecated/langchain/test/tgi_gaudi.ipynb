{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7b419db2-6701-499c-abfa-1426f155fff5",
   "metadata": {},
   "source": [
    "## Benchmarking RAG pipeline with Redis and LLM using langsmith\n",
    "This notebook provides steps to Benchmark RAG pipeline using Langsmith. The RAG pipeline is implemented using Redis as vector database and llama2-70b-chat-hf model as LLM which is served by Huggingface TGI endpoint</br>\n",
    "Langsmith documentation: https://docs.smith.langchain.com/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e30e3f0f-6200-464e-b429-6b69c44e06b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#All imports\n",
    "import os\n",
    "import uuid\n",
    "from operator import itemgetter\n",
    "from typing import Sequence\n",
    "\n",
    "from langchain_benchmarks import clone_public_dataset, registry\n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings, HuggingFaceHubEmbeddings\n",
    "from langchain_community.vectorstores import Redis\n",
    "from langchain_community.llms import HuggingFaceEndpoint\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain.schema.document import Document\n",
    "from langchain.schema.output_parser import StrOutputParser\n",
    "from langchain.schema.runnable.passthrough import RunnableAssign\n",
    "from transformers import AutoTokenizer, LlamaForCausalLM\n",
    "\n",
    "from langsmith.client import Client\n",
    "from langchain_benchmarks.rag import get_eval_config\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c57bae87-2582-419d-8dcf-66c342594ae5",
   "metadata": {},
   "source": [
    "### Configuration parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8989b9cf-ff52-4d10-941d-e43beb4678a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Configuration parameters\n",
    "\n",
    "os.environ[\"LANGCHAIN_ENDPOINT\"] = \"https://api.smith.langchain.com\"\n",
    "os.environ[\"LANGCHAIN_API_KEY\"] = \"add-your-langsmith-key\"  # Your API key\n",
    "\n",
    "#Vector DB configuration\n",
    "EMBED_MODEL = \"\" #Huggingface sentencetransformer model that you want to use. ex. \"BAAI/bge-base-en-v1.5\"\n",
    "REDIS_INDEX_NAME = \"\"  #Name of the index to be created in DB\n",
    "REDIS_SERVER_URL = \"\"  #Specify url of your redis server\n",
    "REDIS_INDEX_SCHEMA = \"\" #path to redis schema yml file. Schema to stor data, vectors and desired metadata for every entry\n",
    "\n",
    "#Endpoints\n",
    "TEI_ENDPOINT = \"Add your TEI endpoint\" #Huggingface TEI endpoint url for Embedding model serving. Make sure TEI is serving the same EMBED_MODEL specified above\n",
    "TGI_ENDPOINT = \"Add your TGI endpoint\" #Huggingface TGI endpoint url for Embedding model serving\n",
    "VLLM_ENDPOINT = \"Add your VLLM endpoint\" #vllm server endpoint (either this or TGI_ENDPOINT should be specified)\n",
    "METHOD = \"\" #give \"tgi-gaudi\" to use TGI_ENDPOINT or \"vllm-openai\" to use VLLM_ENDPOINT\n",
    "\n",
    "#Test parameters\n",
    "LANGSMITH_PROJECT_NAME = \"\" #The test result will be displayed in langsmith cloud with this project name and an unique uuid\n",
    "CONCURRENCY_LEVEL = 16 #Number of concurrent queries to be sent to RAG chain\n",
    "LANGCHAIN_DATASET_NAME = 'LangChain Docs Q&A' #Specify the Langchain dataset name (if using a dataset from langchain)\n",
    "\n",
    "#LLM parameters\n",
    "LLM_MODEL_NAME = \"meta-llama/Llama-2-70b-chat-hf\"\n",
    "MAX_OUTPUT_TOKENS = 128\n",
    "PROMPT_TOKENS_LEN=214 # Magic number for prompt template tokens. This changes if prompt changes\n",
    "MAX_INPUT_TOKENS=1024 #Use this and PROMPT_TOKENS_LEN if there is a need to limit input tokens."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4e430ea-83e1-417f-a6fc-57a3ffdcac85",
   "metadata": {},
   "source": [
    "### Selecting dataset\n",
    "Below section covers selecting and using LangChain Docs Q&A dataset\n",
    "This can be modified to use any dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "667c6870-10b7-492b-bc09-fddf0b1e3d76",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<thead>\n",
       "<tr><th>Name                   </th><th>Type         </th><th>Dataset ID                                                                                                                                                 </th><th>Description  </th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>LangChain Docs Q&A     </td><td>RetrievalTask</td><td><a href=\"https://smith.langchain.com/public/452ccafc-18e1-4314-885b-edd735f17b9d/d\" target=\"_blank\" rel=\"noopener\">452ccafc-18e1-4314-885b-edd735f17b9d</a></td><td>Questions and answers based on a snapshot of the LangChain python docs.\n",
       "\n",
       "The environment provides the documents and the retriever information.\n",
       "\n",
       "Each example is composed of a question and reference answer.\n",
       "\n",
       "Success is measured based on the accuracy of the answer relative to the reference answer.\n",
       "We also measure the faithfulness of the model's response relative to the retrieved documents (if any).              </td></tr>\n",
       "<tr><td>Semi-structured Reports</td><td>RetrievalTask</td><td><a href=\"https://smith.langchain.com/public/c47d9617-ab99-4d6e-a6e6-92b8daf85a7d/d\" target=\"_blank\" rel=\"noopener\">c47d9617-ab99-4d6e-a6e6-92b8daf85a7d</a></td><td>Questions and answers based on PDFs containing tables and charts.\n",
       "\n",
       "The task provides the raw documents as well as factory methods to easily index them\n",
       "and create a retriever.\n",
       "\n",
       "Each example is composed of a question and reference answer.\n",
       "\n",
       "Success is measured based on the accuracy of the answer relative to the reference answer.\n",
       "We also measure the faithfulness of the model's response relative to the retrieved documents (if any).              </td></tr>\n",
       "<tr><td>Multi-modal slide decks</td><td>RetrievalTask</td><td><a href=\"https://smith.langchain.com/public/40afc8e7-9d7e-44ed-8971-2cae1eb59731/d\" target=\"_blank\" rel=\"noopener\">40afc8e7-9d7e-44ed-8971-2cae1eb59731</a></td><td>This public dataset is a work-in-progress and will be extended over time.\n",
       "        \n",
       "Questions and answers based on slide decks containing visual tables and charts.\n",
       "\n",
       "Each example is composed of a question and reference answer.\n",
       "\n",
       "Success is measured based on the accuracy of the answer relative to the reference answer.              </td></tr>\n",
       "</tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "Registry(tasks=[RetrievalTask(name='LangChain Docs Q&A', dataset_id='https://smith.langchain.com/public/452ccafc-18e1-4314-885b-edd735f17b9d/d', description=\"Questions and answers based on a snapshot of the LangChain python docs.\\n\\nThe environment provides the documents and the retriever information.\\n\\nEach example is composed of a question and reference answer.\\n\\nSuccess is measured based on the accuracy of the answer relative to the reference answer.\\nWe also measure the faithfulness of the model's response relative to the retrieved documents (if any).\\n\", get_docs=<function load_cached_docs at 0x7f1408092c00>, retriever_factories={'basic': <function _chroma_retriever_factory at 0x7f14076637e0>, 'parent-doc': <function _chroma_parent_document_retriever_factory at 0x7f1407663880>, 'hyde': <function _chroma_hyde_retriever_factory at 0x7f1407663920>}, architecture_factories={'conversational-retrieval-qa': <function default_response_chain at 0x7f1408092b60>}), RetrievalTask(name='Semi-structured Reports', dataset_id='https://smith.langchain.com/public/c47d9617-ab99-4d6e-a6e6-92b8daf85a7d/d', description=\"Questions and answers based on PDFs containing tables and charts.\\n\\nThe task provides the raw documents as well as factory methods to easily index them\\nand create a retriever.\\n\\nEach example is composed of a question and reference answer.\\n\\nSuccess is measured based on the accuracy of the answer relative to the reference answer.\\nWe also measure the faithfulness of the model's response relative to the retrieved documents (if any).\\n\", get_docs=<function load_docs at 0x7f14076944a0>, retriever_factories={'basic': <function _chroma_retriever_factory at 0x7f1407694540>, 'parent-doc': <function _chroma_parent_document_retriever_factory at 0x7f14076945e0>, 'hyde': <function _chroma_hyde_retriever_factory at 0x7f1407694680>}, architecture_factories={}), RetrievalTask(name='Multi-modal slide decks', dataset_id='https://smith.langchain.com/public/40afc8e7-9d7e-44ed-8971-2cae1eb59731/d', description='This public dataset is a work-in-progress and will be extended over time.\\n        \\nQuestions and answers based on slide decks containing visual tables and charts.\\n\\nEach example is composed of a question and reference answer.\\n\\nSuccess is measured based on the accuracy of the answer relative to the reference answer.\\n', get_docs={}, retriever_factories={}, architecture_factories={})])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Langchain supported datasets for Retrieval task\n",
    "registry = registry.filter(Type=\"RetrievalTask\")\n",
    "registry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "81c8d507-40d5-4f56-9b68-6f579aa6cce7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tbody>\n",
       "<tr><td>Name                  </td><td>LangChain Docs Q&A                                                                                                                                         </td></tr>\n",
       "<tr><td>Type                  </td><td>RetrievalTask                                                                                                                                              </td></tr>\n",
       "<tr><td>Dataset ID            </td><td><a href=\"https://smith.langchain.com/public/452ccafc-18e1-4314-885b-edd735f17b9d/d\" target=\"_blank\" rel=\"noopener\">452ccafc-18e1-4314-885b-edd735f17b9d</a></td></tr>\n",
       "<tr><td>Description           </td><td>Questions and answers based on a snapshot of the LangChain python docs.\n",
       "\n",
       "The environment provides the documents and the retriever information.\n",
       "\n",
       "Each example is composed of a question and reference answer.\n",
       "\n",
       "Success is measured based on the accuracy of the answer relative to the reference answer.\n",
       "We also measure the faithfulness of the model's response relative to the retrieved documents (if any).                                                                                                                                                            </td></tr>\n",
       "<tr><td>Retriever Factories   </td><td>basic, parent-doc, hyde                                                                                                                                    </td></tr>\n",
       "<tr><td>Architecture Factories</td><td>conversational-retrieval-qa                                                                                                                                </td></tr>\n",
       "<tr><td>get_docs              </td><td><function load_cached_docs at 0x7f1408092c00>                                                                                                              </td></tr>\n",
       "</tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "RetrievalTask(name='LangChain Docs Q&A', dataset_id='https://smith.langchain.com/public/452ccafc-18e1-4314-885b-edd735f17b9d/d', description=\"Questions and answers based on a snapshot of the LangChain python docs.\\n\\nThe environment provides the documents and the retriever information.\\n\\nEach example is composed of a question and reference answer.\\n\\nSuccess is measured based on the accuracy of the answer relative to the reference answer.\\nWe also measure the faithfulness of the model's response relative to the retrieved documents (if any).\\n\", get_docs=<function load_cached_docs at 0x7f1408092c00>, retriever_factories={'basic': <function _chroma_retriever_factory at 0x7f14076637e0>, 'parent-doc': <function _chroma_parent_document_retriever_factory at 0x7f1407663880>, 'hyde': <function _chroma_hyde_retriever_factory at 0x7f1407663920>}, architecture_factories={'conversational-retrieval-qa': <function default_response_chain at 0x7f1408092b60>})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Lets use LangChain Docs Q&A dataset for our benchmark\n",
    "langchain_docs = registry[LANGCHAIN_DATASET_NAME]\n",
    "langchain_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bece8e4b-2fd7-4483-abce-2f37ebf858a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset LangChain Docs Q&A already exists. Skipping.\n",
      "You can access the dataset at https://smith.langchain.com/o/9534e90b-1d2b-55ed-bf79-31dc5ff16722/datasets/3ce3b4a1-0640-4fbf-925e-2c03caceb5ac.\n"
     ]
    }
   ],
   "source": [
    "#Download the dataset locally\n",
    "clone_public_dataset(langchain_docs.dataset_id, dataset_name=langchain_docs.name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d816db14-8175-4c9e-9f99-0b877553fdc9",
   "metadata": {},
   "source": [
    "### Ingesting data into Redis vector DB\n",
    "This section needs to be run only when the Redis server doesn't already contain the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80581246-0fcd-4da3-9d45-022b871a787a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Embedding model for ingestion \n",
    "embedder = HuggingFaceEmbeddings(model_name=EMBED_MODEL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25ada515",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Ingest the dataset into vector DB\n",
    "_ = Redis.from_texts(\n",
    "    # appending this little bit can sometimes help with semantic retrieval\n",
    "    # especially with multiple companies\n",
    "    texts=[d.page_content for d in docs],\n",
    "    metadatas=[d.metadata for d in docs],\n",
    "    embedding=embedder,\n",
    "    index_name=REDIS_INDEX_NAME,\n",
    "    index_schema=REDIS_INDEX_SCHEMA,\n",
    "    redis_url=REDIS_SERVER_URL,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4c2f599-da93-4700-aec3-4db2d43d1ef8",
   "metadata": {},
   "source": [
    "### RAG pipeline\n",
    "Initialize each component of RAG pipeline and setup the chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5756e0db",
   "metadata": {},
   "outputs": [],
   "source": [
    "#enable TEI endpoint to get high throughput high throughput queries\n",
    "embedder =  HuggingFaceHubEmbeddings(model=TEI_ENDPOINT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7f906c62-2e59-481f-9159-b2dca087d802",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Initialize retriever to be added in langchain RAG chain.\n",
    "vectorstore = Redis.from_existing_index(\n",
    "    embedding=embedder, index_name=REDIS_INDEX_NAME, schema=REDIS_INDEX_SCHEMA, redis_url=REDIS_SERVER_URL\n",
    ")\n",
    "retriever = vectorstore.as_retriever()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdf91d5b-e7e9-41d9-830b-465d87ffc5b0",
   "metadata": {},
   "source": [
    "**Note:** Prompt is specific to dataset. Modify the prompt accordingly based on the dataset selected. </br>\n",
    "The below prompt is for Langchain Docs Q&A dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e9e77e72-fa65-48a6-aae9-1bec9875b124",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Setup prompt\n",
    "\n",
    "#helper function to crop input tokens from retrieved doc from vector DB\n",
    "#This can be used in format_docs function if there is a need to make sure\n",
    "#number of input tokens doesn't exceed certain limit\n",
    "tokenizer = AutoTokenizer.from_pretrained(LLM_MODEL_NAME)\n",
    "def crop_tokens(prompt, max_len):\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
    "    inputs_cropped = inputs['input_ids'][0:,0:max_len]\n",
    "    prompt_cropped=tokenizer.batch_decode(inputs_cropped, skip_special_tokens=True, clean_up_tokenization_spaces=False)[0]\n",
    "    return prompt_cropped\n",
    "\n",
    "# After the retriever fetches documents, this\n",
    "# function formats them in a string to present for the LLM\n",
    "def format_docs(docs: Sequence[Document]) -> str:\n",
    "    formatted_docs = []\n",
    "    for i, doc in enumerate(docs):\n",
    "        doc_string = (\n",
    "            f\"<document index='{i}'>\\n\"\n",
    "            f\"<source>{doc.metadata.get('source')}</source>\\n\"\n",
    "            f\"<doc_content>{doc.page_content}</doc_content>\\n\"\n",
    "            \"</document>\"\n",
    "        )\n",
    "        # Truncate the retrieval data based on the max tokens required\n",
    "        cropped= crop_tokens(doc_string,MAX_INPUT_TOKENS-PROMPT_TOKENS_LEN) #remove this if there is not need of limiting INPUT tokens to LLM\n",
    "        formatted_docs.append(doc_string)\n",
    "    formatted_str = \"\\n\".join(formatted_docs)\n",
    "    return f\"<documents>\\n{formatted_str}\\n</documents>\"\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            \"You are an AI assistant answering questions about LangChain.\"\n",
    "            \"\\n{context}\\n\"\n",
    "            \"Respond solely based on the document content.\",\n",
    "        ),\n",
    "        (\"human\", \"{question}\"),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "eee03d33-7e7b-41b8-8717-b031a49c1a36",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token will not been saved to git credential helper. Pass `add_to_git_credential=True` if you want to set the git credential as well.\n",
      "Token is valid (permission: read).\n",
      "Your token has been saved to /root/.cache/huggingface/token\n",
      "Login successful\n"
     ]
    }
   ],
   "source": [
    "#Setup LLM \n",
    "\n",
    "llm = None\n",
    "match METHOD:\n",
    "    case \"tgi-gaudi\":\n",
    "        llm = HuggingFaceEndpoint(\n",
    "        endpoint_url=TGI_ENDPOINT,\n",
    "        max_new_tokens=MAX_OUTPUT_TOKENS,\n",
    "        top_k=10,\n",
    "        top_p=0.95,\n",
    "        typical_p=0.95,\n",
    "        temperature=1.0,\n",
    "        repetition_penalty=1.03,\n",
    "        streaming=False,\n",
    "        truncate=1024\n",
    "        )\n",
    "    case \"vllm-openai\":\n",
    "        llm = ChatOpenAI(\n",
    "        model=LLM_MODEL_NAME,\n",
    "        openai_api_key=\"EMPTY\", \n",
    "        openai_api_base=VLLM_ENDPOINT,\n",
    "        max_tokens=MAX_OUTPUT_TOKENS,\n",
    "        temperature=1.0,\n",
    "        top_p=0.95,\n",
    "        streaming=False,\n",
    "        frequency_penalty=1.03\n",
    "        )\n",
    "\n",
    "response_generator = (prompt | llm | StrOutputParser()).with_config(\n",
    "    run_name=\"GenerateResponse\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7164b12b-11e9-4cec-946a-e1902be507da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the final response chain.\n",
    "# It fetches the \"question\" key from the input dict,\n",
    "# passes it to the retriever, then formats as a string.\n",
    "\n",
    "chain = (\n",
    "    RunnableAssign(\n",
    "        {\n",
    "            \"context\": (itemgetter(\"question\") | retriever | format_docs).with_config(\n",
    "                run_name=\"FormatDocs\"\n",
    "            )\n",
    "        }\n",
    "    )\n",
    "    # The \"RunnableAssign\" above returns a dict with keys\n",
    "    # question (from the original input) and\n",
    "    # context: the string-formatted docs.\n",
    "    # This is passed to the response_generator above\n",
    "    | response_generator\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f92a667-9904-437f-88d7-45caf56afbb9",
   "metadata": {},
   "source": [
    "### Setup and run Langsmith benchmark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b39849d7-4b18-4dc5-a97c-f50f528bc980",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Initialize Langchain client\n",
    "client = Client()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9aabc054-7371-4daf-b2a9-3a8ed891e03b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate a unique run ID for this experiment\n",
    "run_uid = uuid.uuid4().hex[:6]\n",
    "\n",
    "#Run the test\n",
    "test_run = client.run_on_dataset(\n",
    "    dataset_name=LANGCHAIN_DATASET_NAME,\n",
    "    llm_or_chain_factory=chain,\n",
    "    evaluation=None,\n",
    "    project_name=LANGSMITH_PROJECT_NAME+'_'+run_uid,\n",
    "    project_metadata={\n",
    "        \"index_method\": \"basic\",\n",
    "    },\n",
    "    concurrency_level=CONCURRENCY_LEVEL,\n",
    "    verbose=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8198756-0797-49ad-8072-1a1d61536689",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
